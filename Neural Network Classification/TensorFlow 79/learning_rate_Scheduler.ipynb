{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's a breakdown of what your learning rate scheduler does:\n",
    "\n",
    "**lambda epoch**: 1e-4 * 10**(epoch/20): This lambda function takes an epoch number as input and returns a learning rate. The formula 1e-4 * 10**(epoch/20) is used to calculate the learning rate for each epoch.\n",
    "\n",
    "**epoch**: The current epoch number during training.\n",
    "\n",
    "10**(epoch/20): This term increases exponentially with the epoch number. As the epochs increase, the learning rate grows significantly.\n",
    "\n",
    "**1e-4**: This is a scaling factor. It determines the initial learning rate at the first epoch. In this case, it's set to 0.0001.\n",
    "\n",
    "So, at the beginning of training (epoch 0), the learning rate will be 1e-4 * 10**(0/20) = 1e-4 * 10**0 = 1e-4. As the epochs progress, the learning rate will increase exponentially. For example, at epoch 20, the learning rate will be 1e-4 * 10**(20/20) = 1e-3, and at epoch 40, it will be 1e-4 * 10**(40/20) = 1e-2, and so on.\n",
    "\n",
    "This type of learning rate scheduling can be helpful in finding an optimal learning rate for your training process. It allows you to start with a small learning rate and then increase it gradually to potentially speed up convergence. However, it's important to monitor the training process and adjust the learning rate schedule as needed, as overly aggressive learning rate increases can lead to instability or failure to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's a breakdown of what the plotting function code does:\n",
    "\n",
    "Calculate an array of learning rates (lrs) using the same formula you used in the learning rate scheduler: 1e-4 * (10**(np.arange(100)/20)).\n",
    "\n",
    "Create a Matplotlib figure with the specified figure size.\n",
    "\n",
    "Use plt.semilogx() to create a semilog plot, where the x-axis (learning rate) is on a logarithmic scale. This is important because learning rates can vary widely, and a logarithmic scale can help visualize this range effectively.\n",
    "\n",
    "Provide the learning rates (lrs) as the x-values and the loss values from your training history (history.history[\"loss\"]) as the y-values for the plot.\n",
    "\n",
    "Set labels for the x-axis, y-axis, and the title of the plot."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
