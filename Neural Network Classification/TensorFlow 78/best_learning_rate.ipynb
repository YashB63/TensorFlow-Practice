{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best learning rate\n",
    "\n",
    "Aside from the architecture itself (the layers, number of neurons, activations, etc), yhe most important hyperparameter you can tune for your neural network models is the learning rate.\n",
    "\n",
    "In the previious model you saw  we lowered the Adam optimizer's learning rate from the default of 0.001 (default) to 0.01.\n",
    "\n",
    "And you might be wondering why we did this.\n",
    "\n",
    "Put it in this way, it was a lucky guess.\n",
    "\n",
    "We just decided to try a lower learning_rate and see hoe the model went.\n",
    "\n",
    "Now you might be thinking, \"Seriously? you can do that?\"\n",
    "\n",
    "And the answer is YES. You can change any of the hyperparameters of your neural networks.\n",
    "\n",
    "With practice, you'll start to see what kind of hyperparameters work and what don't.\n",
    "\n",
    "That's an important thing to understand about machine learning and deep learning in general. It's very experimental. You build a model and evaluate it, build a model and evaluate it.\n",
    "\n",
    "That being said, I want to introduce you a trick which will help you find the optimal learning rate (at least to begin training with) for your models going forward.\n",
    "\n",
    "To do so, we are going to use the following:\n",
    "\n",
    "    -  A learning rate callback.\n",
    "        -  You can think of a callback as an extra piece of functionality you can add to your model white it's training.\n",
    "    -  Another model (we could use the same ones as above, we are practicing building models here).\n",
    "    - A modified loss curves plot.\n",
    "\n",
    "We will go through each with code, then explain what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The default hyperparameters of many neural network building blocks in TensorFlow are setup in such a way which usually work right out of the box(eg: the Adam optimizer's default settings can usually get good results on many datasets). So, it's a good idea try the defaults first, then adjust as needed."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
