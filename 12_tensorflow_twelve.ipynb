{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the Datatype of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new tensor with default datatype (float32)\n",
    "B = tf.constant([2.7,5.6])\n",
    "B.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 7, 10])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = tf.constant([7,10])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 32-bit precision tensor, often referred to as a single-precision tensor, represents each \n",
    "# numerical value using 32 bits. It provides higher precision and a wider range of representable values \n",
    "# compared to a 16-bit precision tensor.\n",
    "\n",
    "# On the other hand, a 16-bit precision tensor, often referred to as a half-precision tensor, \n",
    "# uses 16 bits to represent each numerical value. It offers reduced precision but consumes less memory \n",
    "# and can potentially lead to faster computations compared to a 32-bit tensor.\n",
    "\n",
    "# Here are some key differences between 32-bit and 16-bit precision tensors:\n",
    "\n",
    "# Precision: The main difference between the two is the precision of the numerical values they can represent. \n",
    "# A 32-bit tensor provides higher precision and can represent a wider range of values with more decimal places,\n",
    "# while a 16-bit tensor has reduced precision and can represent fewer decimal places accurately.\n",
    "\n",
    "# Memory Usage: A 16-bit tensor requires half the memory compared to a 32-bit tensor for storing the same number \n",
    "# of values. This reduced memory usage is beneficial when dealing with large-scale models or datasets \n",
    "# that have memory constraints.\n",
    "\n",
    "# Computation Speed: Operations involving 16-bit tensors can potentially be faster compared to those \n",
    "# involving 32-bit tensors. This is because the reduced precision allows for more efficient memory access \n",
    "# and computation. However, the actual speedup may vary depending on the specific hardware and operations \n",
    "# being performed.\n",
    "\n",
    "# Numerical Stability: Due to the reduced precision, 16-bit tensors can be more susceptible to \n",
    "# numerical instability, especially in scenarios where computations involve very small or \n",
    "# very large values or when performing iterative calculations. This can result in numerical errors \n",
    "# or loss of precision in the final results.\n",
    "\n",
    "# In some cases, using 16-bit precision tensors can be advantageous, such as in deep learning applications\n",
    "# where memory usage and computational speed are critical, and the reduced precision does not \n",
    "# significantly impact the model's performance. However, it's important to consider the trade-off between \n",
    "# precision and potential numerical issues when choosing between 32-bit and 16-bit precision tensors, \n",
    "# ensuring that the chosen precision meets the requirements of the specific application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float16, numpy=array([2.7, 5.6], dtype=float16)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change from float 32 to float 16\n",
    "D = tf.cast(B, dtype=tf.float16)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 7., 10.], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change from int 32 to float 32\n",
    "E = tf.cast(C, dtype=tf.float32)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float16, numpy=array([ 7., 10.], dtype=float16)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = tf.cast(E, dtype=tf.float16)\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
